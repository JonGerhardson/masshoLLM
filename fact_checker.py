"""
Fact checking module for Massachusetts Government Agent.
This module provides verification of LLM-generated summaries against source material
to ensure factual accuracy.
"""
import logging
import json
import re
from typing import Dict, Any, Optional, List
from difflib import SequenceMatcher

class FactChecker:
    """
    Verifies the factual accuracy of LLM-generated summaries against source text.
    """
    
    def __init__(self):
        self.logger = logging.getLogger('fact_checker')
    
    def verify_summary_accuracy(self, source_text: str, generated_summary: str, url: str = "") -> Dict[str, Any]:
        """
        Verify the accuracy of a generated summary against the source text.
        
        Args:
            source_text: The original text that was processed
            generated_summary: The summary generated by the LLM
            url: The URL of the source for logging purposes
            
        Returns:
            Dictionary containing verification results and accuracy score
        """
        if not source_text or not generated_summary:
            return {
                'accuracy_score': 0.0,
                'is_accurate': False,
                'confidence_score': 0.0,
                'issues': ['Missing source text or generated summary'],
                'suggestions': [],
                'details': {}
            }
        
        # Perform multiple verification checks
        content_alignment_check = self._check_content_alignment(source_text, generated_summary)
        quote_accuracy_check = self._check_quote_accuracy(source_text, generated_summary)
        factual_consistency_check = self._check_factual_consistency(source_text, generated_summary)
        omission_level_check = self._check_omission_level(source_text, generated_summary)
        
        checks = [content_alignment_check, quote_accuracy_check, factual_consistency_check, omission_level_check]
        
        # Aggregate results
        accuracy_score = self._calculate_accuracy_score(checks)
        issues = self._aggregate_issues(checks)
        suggestions = self._generate_suggestions(source_text, generated_summary, issues)
        
        is_accurate = accuracy_score >= 0.7  # Threshold for acceptable accuracy
        
        result = {
            'accuracy_score': accuracy_score,
            'is_accurate': is_accurate,
            'confidence_score': accuracy_score,  # Using accuracy as confidence for now
            'issues': issues,
            'suggestions': suggestions,
            'source_text_length': len(source_text),
            'summary_length': len(generated_summary),
            'details': {
                'content_alignment': content_alignment_check['details'],
                'quote_accuracy': quote_accuracy_check['details'],
                'factual_consistency': factual_consistency_check['details'],
                'omission_level': omission_level_check['details']
            }
        }
        
        if not is_accurate:
            self.logger.warning(f"Low accuracy summary detected for {url}: {accuracy_score:.2f} - Issues: {issues}")
        
        return result
    
    def _check_content_alignment(self, source: str, summary: str) -> Dict[str, Any]:
        """
        Check how well the summary aligns with the source content.
        """
        # Calculate similarity ratio
        similarity = SequenceMatcher(None, source.lower(), summary.lower()).ratio()
        
        # Extract key phrases from summary and check if they appear in source
        summary_phrases = self._extract_key_phrases(summary)
        found_phrases = [phrase for phrase in summary_phrases 
                        if self._phrase_exists_in_source(phrase, source)]
        
        phrase_alignment = len(found_phrases) / len(summary_phrases) if summary_phrases else 1.0
        
        # Combine metrics
        alignment_score = (similarity + phrase_alignment) / 2
        
        issues = []
        if alignment_score < 0.3:
            issues.append("Low content alignment between summary and source")
        elif alignment_score < 0.7:
            issues.append("Moderate content alignment - verify key claims")
        
        return {
            'score': alignment_score,
            'issues': issues,
            'details': {
                'similarity_ratio': similarity,
                'phrase_alignment': phrase_alignment,
                'matched_phrases': found_phrases
            }
        }
    
    def _check_quote_accuracy(self, source: str, summary: str) -> Dict[str, Any]:
        """
        Check if quoted content in the summary appears verbatim in the source.
        """
        # Find quoted strings in the summary
        quoted_matches = re.findall(r'"([^"]*)"', summary)
        
        if not quoted_matches:
            return {
                'score': 1.0,  # No quotes to verify
                'issues': [],
                'details': {'verified_quotes': []}
            }
        
        verified_quotes = []
        unverified_quotes = []
        
        for quote in quoted_matches:
            if len(quote.strip()) > 5:  # Only check quotes with substantial content
                if self._exact_quote_exists(quote, source):
                    verified_quotes.append(quote)
                else:
                    unverified_quotes.append(quote)
        
        if unverified_quotes:
            accuracy_score = len(verified_quotes) / len(quoted_matches)
            return {
                'score': accuracy_score,
                'issues': [f"Found {len(unverified_quotes)} unverified quotes in summary"],
                'details': {
                    'verified_quotes': verified_quotes,
                    'unverified_quotes': unverified_quotes
                }
            }
        else:
            return {
                'score': 1.0,
                'issues': [],
                'details': {'verified_quotes': verified_quotes}
            }
    
    def _check_factual_consistency(self, source: str, summary: str) -> Dict[str, Any]:
        """
        Check for factual consistency between source and summary.
        """
        # Look for specific patterns that might indicate factual inconsistencies
        issues = []
        
        # Check for numerical values
        source_numbers = re.findall(r'\b\d{4}\b', source)  # Years
        summary_numbers = re.findall(r'\b\d{4}\b', summary)  # Years
        year_consistency = len(set(source_numbers) & set(summary_numbers)) / len(set(source_numbers)) if source_numbers else 1.0
        
        # Check for named entities (simple approach)
        source_entities = self._extract_entities(source)
        summary_entities = self._extract_entities(summary)
        entity_coverage = len(set(summary_entities) & set(source_entities)) / len(set(source_entities)) if source_entities else 1.0
        
        # Calculate consistency score
        consistency_score = (year_consistency + entity_coverage) / 2
        
        if consistency_score < 0.5:
            issues.append("Potential factual inconsistencies detected")
        
        return {
            'score': consistency_score,
            'issues': issues,
            'details': {
                'year_consistency': year_consistency,
                'entity_coverage': entity_coverage,
                'source_entities': source_entities[:10],  # Limit output
                'summary_entities': summary_entities
            }
        }
    
    def _check_omission_level(self, source: str, summary: str) -> Dict[str, Any]:
        """
        Check if the summary omits critical information from the source.
        """
        # Simple approach: check summary length relative to source
        if len(source) == 0:
            return {
                'score': 1.0,
                'issues': [],
                'details': {'omission_level': 0}
            }
        
        compression_ratio = len(summary) / len(source)
        
        # Check if compression is so severe that important info might be lost
        if compression_ratio < 0.05:  # Less than 5% of original length
            return {
                'score': 0.5,
                'issues': ["Summary may omit important information (too short)"],
                'details': {'compression_ratio': compression_ratio}
            }
        elif compression_ratio < 0.1:  # Less than 10% of original length
            return {
                'score': 0.7,
                'issues': ["Summary is quite brief, verify important details not omitted"],
                'details': {'compression_ratio': compression_ratio}
            }
        else:
            return {
                'score': 1.0,
                'issues': [],
                'details': {'compression_ratio': compression_ratio}
            }
    
    def _calculate_accuracy_score(self, checks: List[Dict[str, Any]]) -> float:
        """
        Calculate overall accuracy score from individual checks.
        """
        if not checks:
            return 0.0
        
        total_score = sum(check['score'] for check in checks)
        return total_score / len(checks)
    
    def _aggregate_issues(self, checks: List[Dict[str, Any]]) -> List[str]:
        """
        Aggregate issues from all checks.
        """
        issues = []
        for check in checks:
            issues.extend(check['issues'])
        return list(set(issues))  # Remove duplicates
    
    def _generate_suggestions(self, source: str, summary: str, issues: List[str]) -> List[str]:
        """
        Generate suggestions for improving summary accuracy.
        """
        suggestions = []
        
        if "Low content alignment" in " ".join(issues):
            suggestions.append("Consider re-reading the source text to ensure all key points are captured accurately")
        
        if "unverified quotes" in " ".join(issues):
            suggestions.append("Verify all quoted content appears exactly as in the source document")
        
        if "factual inconsistencies" in " ".join(issues):
            suggestions.append("Cross-check all facts, dates, and figures with the original source")
        
        if "omits important information" in " ".join(issues):
            suggestions.append("Expand the summary to include more key details from the source")
        
        if not suggestions:
            suggestions.append("Summary appears to be factually consistent with source content")
        
        return suggestions
    
    def _extract_key_phrases(self, text: str) -> List[str]:
        """
        Extract key phrases from text for content alignment checking.
        """
        # Simple approach: extract noun phrases and important terms
        # Split by sentences and take significant phrases
        sentences = re.split(r'[.!?]+', text)
        phrases = []
        
        for sentence in sentences:
            # Extract phrases with more than 2 words
            words = sentence.strip().split()
            if len(words) > 3:
                # Take chunks of 2-4 words
                for i in range(len(words) - 1):
                    for length in [2, 3, 4]:
                        if i + length <= len(words):
                            phrase = ' '.join(words[i:i + length])
                            if len(phrase.strip()) > 5:  # Meaningful phrase length
                                phrases.append(phrase.lower())
        
        # Remove duplicates and return
        return list(set(phrases))
    
    def _phrase_exists_in_source(self, phrase: str, source: str) -> bool:
        """
        Check if a phrase exists in the source text (case-insensitive).
        """
        return phrase.lower() in source.lower()
    
    def _exact_quote_exists(self, quote: str, source: str) -> bool:
        """
        Check if an exact quote exists in the source text.
        """
        return quote in source
    
    def _extract_entities(self, text: str) -> List[str]:
        """
        Extract simple named entities (organizations, locations, people).
        """
        # Simple pattern matching for common entity types
        entities = []
        
        # Look for capitalized phrases that might be entities
        # This is a simplified approach - in practice you'd use NER
        words = text.split()
        
        # Look for capitalized consecutive words (potential entities)
        i = 0
        while i < len(words):
            if words[i][0].isupper() and len(words[i]) > 2:
                # Check if next few words are also capitalized (entity phrase)
                entity = [words[i]]
                j = i + 1
                while j < len(words) and words[j][0].isupper() and len(words[j]) > 1:
                    entity.append(words[j])
                    j += 1
                
                if len(entity) >= 1:
                    entity_phrase = ' '.join(entity)
                    # Filter out common words that aren't entities
                    if not any(common in entity_phrase.lower() for common in 
                              ['the', 'and', 'for', 'with', 'from', 'that', 'this', 'when', 'where']):
                        entities.append(entity_phrase)
                
                i = j
            else:
                i += 1
        
        return entities

# Global fact checker instance
fact_checker = FactChecker()

def verify_summary(source_text: str, summary: str, url: str = "") -> Dict[str, Any]:
    """
    Convenience function to verify a summary against source text.
    """
    result = fact_checker.verify_summary_accuracy(source_text, summary, url)
    
    # Create more detailed issue descriptions for better warnings
    detailed_issues = []
    details = result.get('details', {})
    
    # Content alignment issues
    alignment_details = details.get('content_alignment', {})
    if alignment_details.get('similarity_ratio', 1.0) < 0.3:
        detailed_issues.append(f"Low content similarity ({alignment_details.get('similarity_ratio', 0):.2f}) between summary and source")
    
    # Quote accuracy issues
    quote_details = details.get('quote_accuracy', {})
    unverified_quotes = quote_details.get('unverified_quotes', [])
    if unverified_quotes:
        detailed_issues.append(f"Contains {len(unverified_quotes)} unverified quotes: {', '.join(unverified_quotes[:3])}")  # Limit to first 3
    
    # Factual consistency issues
    consistency_details = details.get('factual_consistency', {})
    year_consistency = consistency_details.get('year_consistency', 1.0)
    entity_coverage = consistency_details.get('entity_coverage', 1.0)
    
    if year_consistency < 0.5:
        detailed_issues.append(f"Date/timeline inconsistency (only {year_consistency:.1%} of source dates mentioned)")
    
    if entity_coverage < 0.5:
        detailed_issues.append(f"Entity coverage issue (only {entity_coverage:.1%} of source entities mentioned)")
    
    # Omission issues
    omission_details = details.get('omission_level', {})
    compression_ratio = omission_details.get('compression_ratio', 1.0)
    
    if compression_ratio < 0.05:
        detailed_issues.append("Severe content compression may omit critical information")
    elif compression_ratio < 0.1:
        detailed_issues.append("High content compression may omit important details")
    
    if detailed_issues:
        result['detailed_issues'] = detailed_issues
    
    return result