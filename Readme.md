
# masshoLLM

### Features

Scrapes mass.gov, uses an LLM to summarize new content, and generates daily briefings.

### Quick Start / Installation

Follow these steps to get the agent up and running.

**Prerequisites**

Python 3.9+

Git

**1. Clone the Repository**

```
git clone https://github.com/JonGerhardson/masshoLLM.git
```
```
cd masshoLLM
```

**2. Install Dependencies**

Install all the required Python packages using the `requirements.txt` file.

```
pip install -r requirements.txt
```

**3. Configure the Agent**

Edit the `config.yaml` file to set up the agent. Openrouter, Gemini, or lmstudio. gemini-flash-2.0 has a high daily limit to its free tier and is fast and capable enough for this. 

```
# In config.yaml
llm_settings:

  # ...
  api_keys:
    gemini: "YOUR_GEMINI_API_KEY" # <-- Add your key here
    model: whatever 
```

Alternatively, you can set it as an environment variable, which is more secure:

```
export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
```

**4. Run an Initial Test**

Verify your setup by running the agent in test mode. This will process a small sample of URLs and use a separate `testing.db` database.

```
python main.py --test
```

### Usage

The agent is controlled via the `main.py` script and its command-line arguments.

**Standard Daily Run**

To run the agent for the previous day's updates (the most common use case), simply run:

```
python main.py
```

This will automatically scrape, analyze, generate a report, and create a final news briefing.

**Processing a Specific Date**

Use the `--date` flag to process a specific day's sitemap. Format is YYYY-MM-DD. 

```
python main.py --date 2025-10-10
```

**Standalone Report Generation**

You can re-run the reporting and briefing steps without re-scraping the data.

```
# Re-generate the initial Markdown report
python report_generator.py 2025-10-10

# Re-generate the final news briefing from the initial report
python report_extras.py 2025-10-10
```

**Output Files**

`massgov_updates.db`: The main SQLite database containing all processed data.

`report_YYYY-MM-DD.md`: The initial categorized report with summaries.

`briefing_YYYY-MM-DD.md`: The final, formatted news briefing generated by the LLM.

`briefing_YYYY-MM-DD_full_raw.md`: The raw, combined text sent to the LLM for final formatting.

### Troubleshooting / Tests

**403 Forbidden Errors**

If the scraper is blocked by the server, it will log the failed URLs to a file in the `logs/` directory (e.g., `logs/YYYY-MM-DD_retries.log`). The agent performs a single automatic retry at the end of a normal run. For persistent errors, you can run an aggressive retry with exponential backoff:

```
python main.py --date YYYY-MM-DD --retry
```

**LLM API Failures**

If the LLM call fails for some items, they will be marked in the database. You can re-run the LLM analysis on only the failed items without re-scraping:

```
python main.py --date YYYY-MM-DD --retry_llm
```

**Checking Logs**

The `logs/` directory is essential for debugging.

`agent_run.log`: High-level log of a standard run.

`YYYY-MM-DD_llm.log`: A detailed record of every prompt and response to and from the LLM.

`YYYY-MM-DD_retries.log`: A list of URLs that failed due to `403` errors.

### report_extras

Gussies up the report into a fancy news brief. 

```
python report_extras.py YYYY-MM-DD
```
