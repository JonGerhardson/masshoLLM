
# masshoLLM

## Features

Scrapes mass.gov, uses an LLM to summarize new content, and generates daily briefings.

## Quick Start / Installation

Follow these steps to get the agent up and running.

### Prerequisites

* Python 3.9+
* Git

### 1. Clone the Repository

```
git clone https://github.com/JonGerhardson/masshoLLM.git
cd masshoLLM
```

### 2. Install Dependencies

Install all the required Python packages using the `requirements.txt` file.

```
pip install -r requirements.txt
```

### 3. Configure the Agent

Edit the `config.yaml` file to set up the agent. You can use Openrouter, Gemini, or LM Studio. The free tier of `gemini-flash-2.0` is fast, basically good enough, and has a high daily limit.

In `config.yaml`:

```
llm_settings:
  # ...
  api_keys:
    gemini: "YOUR_GEMINI_API_KEY" # <-- Add your key here
  model: 'gemini-2.5-flash'
```

Alternatively, you can set it as an environment variable, which is more secure:

```
export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
```

### 4. Run an Initial Test

Verify your setup by running the agent in test mode. This will process a small sample of URLs and use a separate `testing.db` database.

```
python main.py --test
```

## Usage

The agent is controlled via the `main.py` script and its command-line arguments.

### Standard Daily Run

To run the agent for the previous day's updates (the most common use case), simply run:

```
python main.py
```

This will automatically scrape, analyze, generate a report, and create a final news briefing.

### Processing a Specific Date

Use the `--date` flag to process a specific day's sitemap. The format is YYYY-MM-DD.

```
python main.py --date 2025-10-10
```

### Standalone Report Generation

You can re-run the reporting and briefing steps without re-scraping the data.

Re-generate the initial Markdown report:

```
python report_generator.py 2025-10-10
```

Re-generate the final news briefing from the database records:

```
python report_extras.py 2025-10-10
```

## Output Files

* **massgov_updates.db**: The main SQLite database containing all processed data.
* **report_YYYY-MM-DD.md**: The initial categorized report with summaries.
* **briefing_YYYY-MM-DD.md**: The final, formatted news briefing generated by the LLM.
* **briefing_YYYY-MM-DD_full_raw.md**: The raw, combined text sent to the LLM for final formatting.

## Troubleshooting / Tests

### 403 Forbidden Errors

If the scraper is blocked, it logs the failed URLs to `logs/YYYY-MM-DD_retries.log`. For persistent errors, you can run an aggressive retry with exponential backoff:

```
python main.py --date YYYY-MM-DD --retry
```

### LLM API Failures

If the LLM call fails for some items, they will be marked in the database. You can re-run the LLM analysis on only the failed items without re-scraping:

```
python main.py --date YYYY-MM-DD --retry_llm
```

### Checking Logs

The `logs/` directory is essential for debugging.

* **agent_run.log**: High-level log of a standard run.
* **YYYY-MM-DD_llm.log**: Detailed record of LLM prompts and responses.
* **YYYY-MM-DD_retries.log**: List of URLs that failed due to 403 errors.
```
